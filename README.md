# YouTube Trending Data Pipeline

## üìù Description

Ce projet met en ≈ìuvre un pipeline de donn√©es pour r√©cup√©rer les donn√©es des vid√©os tendance de l'API YouTube, les traiter en temps quasi r√©el et les stocker pour une analyse ult√©rieure. L'ensemble de l'infrastructure est conteneuris√© √† l'aide de Docker et orchestr√© avec Docker Compose.

## üèóÔ∏è Architecture

Le flux de donn√©es passe par les composants suivants :

1.  **`youtube_producer` (Python)** : Un script Python qui r√©cup√®re p√©riodiquement les donn√©es des vid√©os tendance depuis l'API YouTube v3.
2.  **`kafka` (Confluent Platform)** : Le producteur envoie les donn√©es brutes √† un topic Kafka nomm√© `youtube_trending`. Kafka sert de courtier de messages, d√©couplant l'ingestion du traitement des donn√©es.
3.  **`spark` (Apache Spark)** : Un job Spark consomme les donn√©es du topic Kafka. Il effectue des transformations, nettoie les donn√©es et les charge dans une base de donn√©es PostgreSQL.
4.  **`postgres` (PostgreSQL)** : Une base de donn√©es relationnelle utilis√©e pour stocker les donn√©es vid√©o trait√©es et structur√©es.
5.  **`pgadmin` (pgAdmin 4)** : Une interface graphique web pour g√©rer et interroger les donn√©es stock√©es dans la base de donn√©es PostgreSQL.
6.  **`zookeeper`** : N√©cessaire pour la gestion du cluster Kafka.

## ‚ú® Fonctionnalit√©s

*   Ingestion de donn√©es depuis l'API YouTube.
*   Streaming de donn√©es r√©silient et √©volutif avec Kafka.
*   Traitement de donn√©es distribu√© avec Apache Spark.
*   Stockage de donn√©es structur√©es dans une base de donn√©es PostgreSQL.
*   Interface web simple pour la gestion de la base de donn√©es avec pgAdmin.
*   Enti√®rement conteneuris√© et orchestr√© avec Docker Compose pour une configuration facile.

## üõ†Ô∏è Stack Technique

*   **Orchestration** : Docker, Docker Compose
*   **Langage de Programmation** : Python 3.11
*   **Streaming de Donn√©es** : Apache Kafka
*   **Traitement de Donn√©es** : Apache Spark 3.5
*   **Base de Donn√©es** : PostgreSQL 15
*   **API** : YouTube Data API v3

## üöÄ D√©marrage

### Pr√©requis

*   Docker
*   Docker Compose
*   Une cl√© d'API YouTube Data v3.

### Installation & Configuration

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone <votre-url-de-depot>
    cd youtube-trending-data-pipeline
    ```

2.  **Cr√©ez le fichier d'environnement :**
    Cr√©ez un fichier `.env` √† la racine du projet en copiant le fichier d'exemple :
    ```bash
    cp .env.example .env
    ```
    Ensuite, remplissez le fichier `.env` avec vos informations d'identification :
    ```env
    # Cl√© API YouTube
    YOUTUBE_API_KEY=votre_cle_api_youtube

    # Identifiants Postgres
    POSTGRES_USER=votre_utilisateur_postgres
    POSTGRES_PASSWORD=votre_mot_de_passe_postgres
    POSTGRES_DB=votre_nom_de_db

    # Identifiants pgAdmin
    PGADMIN_DEFAULT_EMAIL=votre_email@example.com
    PGADMIN_DEFAULT_PASSWORD=votre_mot_de_passe_pgadmin
    ```

3.  **Construisez et lancez les services :**
    Utilisez Docker Compose pour construire les images et d√©marrer tous les conteneurs en mode d√©tach√©.
    ```bash
    docker-compose up --build -d
    ```

### Utilisation

*   **Pipeline de Donn√©es** : Le pipeline d√©marre automatiquement. Le service `youtube_producer` commencera √† r√©cup√©rer les donn√©es et √† les envoyer √† Kafka, et le service `spark` les traitera pour les stocker dans PostgreSQL.
*   **Acc√©der √† pgAdmin** :
    *   Ouvrez votre navigateur web et allez sur `http://localhost:8080`.
    *   Connectez-vous avec le `PGADMIN_DEFAULT_EMAIL` et le `PGADMIN_DEFAULT_PASSWORD` que vous avez d√©finis dans le fichier `.env`.
    *   Vous devrez ajouter une nouvelle connexion serveur pour acc√©der √† la base de donn√©es PostgreSQL :
        *   **Host name/address** : `postgres` (le nom du service dans `docker-compose.yml`)
        *   **Port** : `5432`
        *   **Maintenance database** : La valeur de `POSTGRES_DB`
        *   **Username** : La valeur de `POSTGRES_USER`
        *   **Password** : La valeur de `POSTGRES_PASSWORD`
*   **Consulter les logs** : Pour voir les logs de tous les services, ex√©cutez :
    ```bash
    docker-compose logs -f
    ```
    Pour suivre les logs d'un service sp√©cifique (par exemple, `spark`) :
    ```bash
    docker-compose logs -f spark
    ```

## üìÅ Structure du Projet

```
.
‚îú‚îÄ‚îÄ Dockerfile.producer     # Dockerfile pour le producteur Python
‚îú‚îÄ‚îÄ Dockerfile.spark        # Dockerfile pour le job Spark
‚îú‚îÄ‚îÄ docker-compose.yml      # Fichier Docker Compose pour orchestrer tous les services
‚îú‚îÄ‚îÄ ingestion/              # Contient le script du producteur Python
‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python pour le producteur
‚îú‚îÄ‚îÄ spark_jobs/             # Contient le script de traitement Spark
‚îú‚îÄ‚îÄ sql/                    # Scripts SQL pour l'initialisation de la BDD
‚îî‚îÄ‚îÄ README.md               # Ce fichier
```

## ‚èπÔ∏è Arr√™t

Pour arr√™ter et supprimer tous les conteneurs, r√©seaux et volumes cr√©√©s par Docker Compose, ex√©cutez la commande suivante depuis la racine du projet :

```bash
docker-compose down
```